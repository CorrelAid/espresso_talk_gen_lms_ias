---
title: "Grundlagen Generative Sprachmodelle und ihr Einsatz "
format: revealjs
date: 04/15/2025
author: Jonas Stettner | CorrelAid @ CDL
theme: cdl_assets/cdl.scss
logo: cdl_assets/logo.png
footer: "civic-data.de"
bibliography: references.bib
csl: cdl_assets/apa.csl
self-contained: true    
---

## KI Begriff: Intelligenz ü§ñ 

> "[I]ntelligent‚Äù according to what definition? The three-stratum definition? Howard Gardner‚Äôs theory of multiple intelligences? The Stanford-Binet Intelligence Scale? Bender remains particularly fond of an alternative name for AI proposed by a former member of the Italian Parliament: ‚ÄúSystematic Approaches to Learning Algorithms and Machine Inferences.‚Äù Then people would be out here asking, ‚ÄúIs this SALAMI intelligent? Can this SALAMI write a novel? Does this SALAMI deserve human rights? [@weil_chatgpt_2023]

## KI Begriff: Definiton 

> Systeme der K√ºnstlichen Intelligenz sind in der Lage, Daten und Informationen auf eine Weise zu verarbeiten, die menschlichen Denkprozessen √§hnelt oder diese nachzuahmen scheint. Dies beinhaltet Aspekte des Denkens und Lernens, der Wahrnehmung, Vorhersage, Planung oder Steuerung [@deutsche_unesco-kommission_zusammenfassung_2023, S. 8]

## Einordnung 

![](images/drawing.png){fig-## Neuronale Netzwerke align="center"}


## Neuronale Netzwerke

- Model biologischer Neuronen, basierend auf dem, was damals √ºber neuronale Aktivit√§t bekannt war [@mcculloch_logical_1943]
- Neruonen sind Recheneinheiten, die in Schichten miteinander verbunden sind, welche die Architektur des Netzwerks definieren
- Das Lernen des Netzwerks wird oft durch sog, Backpropagation geleitet, das die Gewichte anpasst, um den Vorhersagefehler zu minimieren.

## Neuronale Netzwerke: Schaubild

![Glosser.ca, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons](images/Colored_neural_network.png){fig-align="center"}

## Sprachmodelle 

- Ein Modell, dass die Wahrscheinlichkeiten der m√∂glichen n√§chsten W√∂rter ausgibt [@jurafsky_speech_2025]
- ELIZA (1967) als Beispiel f√ºr einen Chatbot mit symbolischer KI (kein Sprachmodell) 

## Training neuronaler Sprachmodelle

![Source: Holistic AI, <https://www.holisticai.com/blog/from-transformer-architecture-to-prompt-engineering>](images/lmodeling.png){fig-align="center"}

- "Transformer" sind eine Architektur neuronaler Netzwerke, die Tokens innerhalb des Inputs kontextualisieren, indem sie deren Beziehungen zueinander ber√ºcksichtigen
- GPT = Generative Pretrained Transformer

## "Gro√üe" Sprachmodelle

- BERT (340 million Parameter) vs GPT-4 (1.8 Billionen Parameter) 
- AI-Scaling-Gesetze: Gr√∂√üere Modelle profitieren von mehr Daten und Rechenleistung (GPU), jedoch mit abnehmendem Grenznutzen.
- Neues LLama 4 "Behemoth" (2 Billionen Parameter) zeigt nicht erwartete Performance
- Entstehender Konsens, dass Performance durch andere Methoden erh√∂ht werden muss (z.B. Reasoning)

## Basis-Modelle

!["Das √ñkosystem der KI-Basismodelle", reframe\[Tech\], Namensnennung-Share Alike 4.0 International, <https://www.reframetech.de/wissensseite-basismodelle/>](images/Oekosystem_Basismodelle.png){fig-align="center"}

- Open vs. propriet√§r 

## Stochastic Parrots ü¶ú



## RAG

## RAG: Quellenangabe

- Quellenangabe ist regul√§rer Teil des Outputs des LLMs
- LLM bekommt neben des Ergebnissen des Suchschritts auch Meta-Informationen, wie z.B. Seitenzahlen
- Ausgabe wird formatiert, sodass Quellenangaben z.B. als hochgestellte Zahl erscheinen


## Agents

## Informationszuganssysteme

## Literaturverzeichnis

::: {#refs}
:::