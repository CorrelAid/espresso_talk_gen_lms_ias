
@misc{gunther_jina_2023,
	title = {Jina {Embeddings} 2: 8192-{Token} {General}-{Purpose} {Text} {Embeddings} for {Long} {Documents}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Jina {Embeddings} 2},
	url = {https://arxiv.org/abs/2310.19923},
	doi = {10.48550/ARXIV.2310.19923},
	abstract = {Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.
 To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only achieves state-of-the-art performance on a range of embedding-related tasks in the MTEB benchmark but also matches the performance of OpenAI's proprietary ada-002 model. Additionally, our experiments indicate that an extended context can enhance performance in tasks such as NarrativeQA.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Günther, Michael and Ong, Jackmin and Mohr, Isabelle and Abdessalem, Alaeddine and Abel, Tanguy and Akram, Mohammad Kalim and Guzman, Susana and Mastrapas, Georgios and Sturua, Saba and Wang, Bo and Werk, Maximilian and Wang, Nan and Xiao, Han},
	year = {2023},
	note = {Version Number: 4},
	keywords = {68T50, Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, I.2.7, Machine Learning (cs.LG)},
}

@book{deutsche_unesco-kommission_zusammenfassung_2023,
	address = {Bonn, Deutschland},
	title = {Zusammenfassung der {UNESCO}-{Empfehlung} zur {Ethik} der {Künstlichen} {Intelligenz}: {Wegweiser} für die {Gestaltung} unserer {Zukunft}},
	isbn = {978-3-947675-34-0},
	url = {https://www.unesco.de/dokumente-und-hintergruende/publikationen/detail/die-unesco-empfehlung-zur-ethik-der-kuenstlichen-intelligenz/},
	language = {German},
	publisher = {Deutsche UNESCO-Kommission e. V.},
	author = {{Deutsche UNESCO-Kommission} and {Niederländische UNESCO-Nationalkommission} and {Slowenische UNESCO-Nationalkommission}},
	editor = {Anne Diessner, Jeannine Hausmann, Maximilian Müngersdorff, Davor Orlic, Jon Verrie},
	translator = {Diessner, Anne},
	year = {2023},
}

@article{weil_chatgpt_2023,
	title = {{ChatGPT} {Is} {Nothing} {Like} a {Human}, {Says} {Linguist} {Emily} {Bender}},
	url = {https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html},
	urldate = {2025-04-08},
	journal = {New York Magazine: Intelligencer},
	author = {Weil, Elizabeth},
	year = {2023},
}

@book{jurafsky_speech_2025,
	edition = {3rd},
	title = {Speech and {Language} {Processing}: {An} {Introduction} to {Natural} {Language} {Processing}, {Computational} {Linguistics}, and {Speech} {Recognition} with {Language} {Models}},
	url = {https://web.stanford.edu/~jurafsky/slp3/},
	author = {Jurafsky, Daniel and Martin, James H.},
	year = {2025},
}
