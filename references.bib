
@misc{gunther_jina_2023,
	title = {Jina {Embeddings} 2: 8192-{Token} {General}-{Purpose} {Text} {Embeddings} for {Long} {Documents}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Jina {Embeddings} 2},
	url = {https://arxiv.org/abs/2310.19923},
	doi = {10.48550/ARXIV.2310.19923},
	abstract = {Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.
 To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only achieves state-of-the-art performance on a range of embedding-related tasks in the MTEB benchmark but also matches the performance of OpenAI's proprietary ada-002 model. Additionally, our experiments indicate that an extended context can enhance performance in tasks such as NarrativeQA.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {G√ºnther, Michael and Ong, Jackmin and Mohr, Isabelle and Abdessalem, Alaeddine and Abel, Tanguy and Akram, Mohammad Kalim and Guzman, Susana and Mastrapas, Georgios and Sturua, Saba and Wang, Bo and Werk, Maximilian and Wang, Nan and Xiao, Han},
	year = {2023},
	note = {Version Number: 4},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, 68T50, Artificial Intelligence (cs.AI), I.2.7, Machine Learning (cs.LG)},
}

@book{deutsche_unesco-kommission_zusammenfassung_2023,
	address = {Bonn, Deutschland},
	title = {Zusammenfassung der {UNESCO}-{Empfehlung} zur {Ethik} der {K√ºnstlichen} {Intelligenz}: {Wegweiser} f√ºr die {Gestaltung} unserer {Zukunft}},
	isbn = {978-3-947675-34-0},
	url = {https://www.unesco.de/dokumente-und-hintergruende/publikationen/detail/die-unesco-empfehlung-zur-ethik-der-kuenstlichen-intelligenz/},
	language = {German},
	publisher = {Deutsche UNESCO-Kommission e. V.},
	author = {{Deutsche UNESCO-Kommission} and {Niederl√§ndische UNESCO-Nationalkommission} and {Slowenische UNESCO-Nationalkommission}},
	editor = {Anne Diessner, Jeannine Hausmann, Maximilian M√ºngersdorff, Davor Orlic, Jon Verrie},
	translator = {Diessner, Anne},
	year = {2023},
}

@article{weil_chatgpt_2023,
	title = {{ChatGPT} {Is} {Nothing} {Like} a {Human}, {Says} {Linguist} {Emily} {Bender}},
	url = {https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html},
	urldate = {2025-04-08},
	journal = {New York Magazine: Intelligencer},
	author = {Weil, Elizabeth},
	year = {2023},
}

@book{jurafsky_speech_2025,
	edition = {3rd},
	title = {Speech and {Language} {Processing}: {An} {Introduction} to {Natural} {Language} {Processing}, {Computational} {Linguistics}, and {Speech} {Recognition} with {Language} {Models}},
	url = {https://web.stanford.edu/~jurafsky/slp3/},
	author = {Jurafsky, Daniel and Martin, James H.},
	year = {2025},
}

@article{mcculloch_logical_1943,
	title = {A {Logical} {Calculus} of the {Ideas} {Immanent} in {Nervous} {Activity}},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the ‚Äúall-or-none‚Äù character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	number = {4},
	urldate = {2024-09-10},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	year = {1943},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation},
	pages = {115--133},
}

@article{edwards_metas_2025,
	title = {Meta‚Äôs surprise {Llama} 4 drop exposes the gap between {AI} ambition and reality},
	url = {https://arstechnica.com/ai/2025/04/metas-surprise-llama-4-drop-exposes-the-gap-between-ai-ambition-and-reality/},
	urldate = {2025-04-09},
	journal = {Ars Technica},
	author = {Edwards, Benj},
	month = apr,
	year = {2025},
}

@article{zeff_current_2024,
	title = {Current {AI} scaling laws are showing diminishing returns, forcing {AI} labs to change course},
	url = {https://techcrunch.com/2024/11/20/ai-scaling-laws-are-showing-diminishing-returns-forcing-ai-labs-to-change-course},
	urldate = {2025-04-09},
	journal = {TechCrunch},
	author = {Zeff, Maxwell},
	month = nov,
	year = {2024},
}

@misc{mccarthy_what_2012,
	title = {What is {AI}? / {Basic} {Questions}},
	url = {http://jmc.stanford.edu/artificial-intelligence/what-is-ai/index.html},
	author = {McCarthy, John},
	year = {2012},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? ü¶ú},
	isbn = {978-1-4503-8309-7},
	url = {https://doi.org/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	year = {2021},
	note = {event-place: Virtual Event, Canada},
	pages = {610--623},
}

@article{mitchell_debate_2023,
	title = {The debate over understanding in {AI}'s large language models},
	volume = {120},
	doi = {10.1073/pnas.2215907120},
	number = {13},
	journal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Mitchell, Melanie and Krakauer, David C.},
	month = mar,
	year = {2023},
	pages = {e2215907120},
}

@misc{owen_how_2024,
	title = {How predictable is language model benchmark performance?},
	url = {https://arxiv.org/abs/2401.04757},
	urldate = {2025-04-14},
	author = {Owen, David},
	year = {2024},
	note = {\_eprint: 2401.04757},
}

@article{assaad_chatgpt_2025,
	title = {{ChatGPT} just passed the {Turing} test. {But} that doesn't mean {AI} is now as smart as humans},
	url = {https://theconversation.com/chatgpt-just-passed-the-turing-test-but-that-doesnt-mean-ai-is-now-as-smart-as-humans-253946},
	journal = {Conversation},
	author = {Assaad, Zena},
	month = apr,
	year = {2025},
}

@inproceedings{bender_climbing_2020,
	address = {Online},
	title = {Climbing towards {NLU}: {On} {Meaning}, {Form}, and {Understanding} in the {Age} of {Data}},
	url = {https://aclanthology.org/2020.acl-main.463/},
	doi = {10.18653/v1/2020.acl-main.463},
	abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as ‚Äúunderstanding‚Äù language or capturing ‚Äúmeaning‚Äù. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of ‚ÄúTaking Stock of Where We`ve Been and Where We`re Going‚Äù, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M. and Koller, Alexander},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {5185--5198},
}

@misc{klein_provocations_2025,
	title = {Provocations from the {Humanities} for {Generative} {AI} {Research}},
	url = {https://arxiv.org/abs/2502.19190},
	author = {Klein, Lauren and Martin, Meredith and Brock, Andr√© and Antoniak, Maria and Walsh, Melanie and Johnson, Jessica Marie and Tilton, Lauren and Mimno, David},
	year = {2025},
	note = {\_eprint: 2502.19190},
}

@article{manning_human_2022,
	title = {Human {Language} {Understanding} \& {Reasoning}},
	volume = {151},
	url = {https://nlp.stanford.edu/~manning/papers/Daedalus_Sp22_09_Manning.pdf},
	number = {2},
	journal = {D√¶dalus},
	author = {Manning, Christopher D.},
	year = {2022},
	pages = {127--138},
}

@article{shah_envisioning_2024,
	title = {Envisioning {Information} {Access} {Systems}: {What} {Makes} for {Good} {Tools} and a {Healthy} {Web}?},
	volume = {18},
	issn = {1559-1131},
	doi = {10.1145/3649468},
	number = {3},
	journal = {ACM Trans. Web},
	author = {Shah, Chirag and Bender, Emily M.},
	month = apr,
	year = {2024},
	note = {Publisher: Association for Computing Machinery},
	pages = {1--24},
}

@article{sypherd_practical_2024,
	title = {Practical {Considerations} for {Agentic} {LLM} {Systems}},
	doi = {10.48550/arXiv.2412.04093},
	journal = {arXiv},
	author = {Sypherd, Chris and Belle, Vaishak},
	month = dec,
	year = {2024},
	note = {\_eprint: 2412.04093},
}

