---
title: "Grundlagen Generative Sprachmodelle und ihr Einsatz "
format: revealjs
date: 04/15/2025
author: Jonas Stettner | CorrelAid @ CDL
theme: cdl_assets/cdl.scss
logo: cdl_assets/logo.png
footer: "civic-data.de"
bibliography: references.bib
csl: cdl_assets/apa.csl
self-contained: true    
---

## Gliederung

1. Grundlagen KI und LLMs
2. Einsatz in Informationszugangssystemen

## Grundlagen KI und LLMs: Gliederung

1. Was ist KI?
2. Neuronale Netzwerke
3. Neuronale Sprachmodelle
4. "Gro√üe" Sprachmodelle
5. LLM Sprachverst√§ndnis

## KI Begriff: Intelligenz ‚Ö† ü§ñ 

> "[I]ntelligent‚Äù according to what definition? The three-stratum definition? Howard Gardner‚Äôs theory of multiple intelligences? [...] [A]n alternative name for AI proposed by a former member of the Italian Parliament: ‚ÄúSystematic Approaches to Learning Algorithms and Machine Inferences.‚Äù Then people would be out here asking, ‚ÄúIs this SALAMI intelligent? Can this SALAMI write a novel? Does this SALAMI deserve human rights? [@weil_chatgpt_2023]

- "Intelligent" als Wertung 

## KI Begriff: Intelligenz ‚Ö°

> Intelligence is the **computational part** of the ability to achieve goals in the world. [...] Intelligence involves mechanisms, and AI research has discovered how to make computers carry out some of them and not others. Such programs should be considered "somewhat intelligent". [@mccarthy_what_2012]

- Intelligenz als auf ein Ziel gerichtete F√§higkeit, die unterschiedliche Komponenten umfasst (z.B. Logik, Kreativit√§t, Erkennen von Objekten)

## KI Begriff: Definiton 

> Systeme der K√ºnstlichen Intelligenz sind in der Lage, Daten und Informationen auf eine Weise zu verarbeiten, die menschlichen Denkprozessen √§hnelt oder diese nachzuahmen scheint. Dies beinhaltet Aspekte des Denkens und Lernens, der Wahrnehmung, Vorhersage, Planung oder Steuerung [@deutsche_unesco-kommission_zusammenfassung_2023, S. 8]

- √Ñhnlichkeit bei Zielen und Herstellung und Funktionsweise von Intelligenz

## KI Tasks 

- KI-Methoden werden f√ºr Tasks entwickelt und mit f√ºr diese gemachten Daten und Metriken evaluiert und verglichen
- Beispiele Tasks: Klassifikation, Textzusammenfassung, Nutzung von Tools

## Einordnung 

![](images/drawing.png){fig-align="center"}

## Neuronale Netzwerke ‚Ö†

- Modell biologischer Neuronen (basierend auf dem, was damals √ºber Neuronen bekannt war)[@mcculloch_logical_1943]

![Illustration eines neuronalen Netzwerks aus Hilary Masons Keynote, <https://www.youtube.com/watch?v=SxxqaC5hf04&t=2394s>](images/artificial-neural-network-cell.png){fig-align="center"}

## Neuronale Netzwerke ‚Ö°
- Neuronen sind Recheneinheiten, die in Schichten auf verschiedene Arten miteinander verbunden sind

![Glosser.ca, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons](images/Colored_neural_network.png){fig-align="center"}

- Deep Learning: Mehr als ein Hidden Layer
- Rechnungen innerhalb des Netzwerks lassen sich effizient auf Grafikkarten ausf√ºhren

## Neuronale Netzwerke ‚Ö¢

- Lernen beim Training durch Backpropagation: 
    - "Forward Pass" resultiert in Output, der mit "Ground Truth" verglichen wird
    - "Backward Pass" mit Korrektur der Parameter (Weights und Biases) basierend auf dem Fehlerwert 
- Neuronale Netzwerke erzielen beste Performance bei vielen Tasks, bzw. erst Erm√∂glichung mancher Tasks

## Sprachmodelle: Definition

- Sprachmodell: Ein Modell, dass die Wahrscheinlichkeiten der m√∂glichen n√§chsten W√∂rter ausgibt [@jurafsky_speech_2025]
- ELIZA (1967) als Beispiel f√ºr einen Chatbot mit symbolischer KI (kein Sprachmodell) 

## Neuronale Sprachmodelle: Training

![Quelle: Holistic AI, <https://www.holisticai.com/blog/from-transformer-architecture-to-prompt-engineering>](images/lmodeling.png){fig-align="center"}

- Vorhersage der Wahrscheinlichkeit eines Tokens in Abh√§ngigkeit von seinem vorangehenden oder umgebenden Kontext
- Transformer Netzwerkarchitektur: Tokens innerhalb des Inputs kontextualisieren (Multi-Head Attention)

## "Gro√üe" Sprachmodelle (LLMs) ‚Ö†

- BERT -> Bidirectional encoder representations from transformers (340 Millionen Parameter) 
    - Finetuning: Anpassung des Modells und Parameter auf Task 
- GPT-4 -> Generative Pre-trained Transformer (gesch√§tzt, nicht offiziell best√§tigt 1.8 Billionen Parameter) 
    - In-Context Learning (ICL) und (Few Shot) Prompting -> Keine Ver√§nderung der Parameter

## "Gro√üe" Sprachmodelle (LLMs) ‚Ö°

- Offene (nicht Open Source) vs Propriet√§re Modelle
- Hugging Face ü§ó als zentraler Ort f√ºr offene Modelle

## "Gro√üe" Sprachmodelle (LLMs) ‚Ö¢
- LLama 4 "Behemoth" (2 Billionen Parameter) zeigt nicht erwartete Performance [@edwards_metas_2025]
- AI-Scaling-Gesetze: Bessere Modelle profitieren von mehr Daten und Rechenleistung (GPU), jedoch mit abnehmendem Grenznutzen. [@owen_how_2024]
- Entstehender Konsens, dass Performance durch andere Methoden erh√∂ht werden muss (z.B. "Reasoning") [@zeff_current_2024]

## LLM Sprachverst√§ndnis ‚Ö†

- LLMs bestehen Turing Test [@assaad_chatgpt_2025]
- Chinese Room: Befolgung syntaktischer Regeln ohne semantisches Verst√§ndnis (Form vs. Bedeutung)

![Quelle: Keno Leon, <https://k3no.medium.com/the-chinese-room-experiment-2c0d63848f05>](images/chinese_room.webp){fig-align="center"}

## LLM Sprachverst√§ndnis ‚Ö°

- Bedeutung entsteht durch Syntax und einer Referenz zu etwas au√üerhalb der Sprache [@bender_climbing_2020] 
    - LLMs "verstehen" Bedeutung nicht, Output an sich hat keine Bedeutung (Stochastic Parrots ü¶ú)
    - Wir interpretieren Output von LLMs nur so, als ob sie Bedeutung vermitteln [@bender_dangers_2021]

> Models make words, but people make meaning. [@klein_provocations_2025]

## LLM Sprachverst√§ndnis ‚Ö¢

- Wiedersprechende Theorie: Bedeutung ergibt sich aus der Art und Weise, wie sich Sprache aufeinander bezieht -> LLMs verstehen Bedeutung [@mitchell_debate_2023; @manning_human_2022]
- Wie passen multimodale Modelle in diese Debatte?
- Alignment-Phase als soziales Lernen?

# Fragen zu den KI-Grundlagen?

## Einsatz von LLMs in Informationszugangssystemen

1. Definition Informationszuganssysteme
2. Informationszugangssystem mit RAG
3. Semantische Suche mit Embeddings
4. Quellenangabe in RAG
5. Tools

## Informationszugangssysteme

> [Information Access] refers to a focused interaction between a person and information where relevant information is sought, found, and used‚Äîwith or without a system. [@shah_envisioning_2024]

- Umfasst Informationsbeschaffung und Informationsfilterung
- Beispiele: Bibliothek, Google, ChatGPT 

## Informationszugangssystem mit RAG

![Turtlecrown, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons](images/RAG_diagram.png){fig-align="center"}

## Semantische Suche mit Embeddings

- Embedding = Repr√§sentation von Sprache z.B. W√∂rtern in Vektorraum
    - Embedding-Modelle sind LLMs, die z.B. f√ºr Suche gefinetuned werden [@gunther_jina_2023]
    - Text-Embedding: Durchschnitt der Token Embeddings im vorherigen Layer resultiert in einem Vektor
- Semantische Suche geschieht √ºber Distanz zwischen Vektoren 

## RAG: Quellenangabe

- Quellenangabe ist regul√§rer Teil des Outputs des LLMs
- LLM bekommt neben des Ergebnissen des Suchschritts auch Meta-Informationen, wie z.B. Seitenzahlen
- Ausgabe wird formatiert, sodass Quellenangaben z.B. als hochgestellte Zahl erscheinen

## LLMs als Agenten

![@sypherd_practical_2024, Figure 2](images/agents.png){fig-align="center"}

## Tools

- Beispiel f√ºr das Abrufen von Tools:

    ```
    {'name': 'add',
    'args': {'a': 11, 'b': 49},
    'id': 'call_VKw8t5tpAuzvbHgdAXe9mjUx'},
    [{'name': 'websearch',
    'args': {'query': 'b√ºrgermeisterin von wesel', 'region': 'germany'},
    'id': 'call_UL7E2232GfDHIQGOM4gJfEDD'},]
    ```
- Statischer Code pr√ºft LLM-Output auf solche Outputs und f√ºhrt Tool-Code aus

## "Agentic" Chatbots

- Websuche als Tool f√ºr LLM-basierte Chatbots wie ChatGPT
- Chatbots als modulare Systeme (Beispiel Migrationsberatung mit Lupai)
    - Spezialisierte Modelle als Module 
    - Festgelegte Logik vs. Steuerung durch LLM
    - Language Detection als fester erster Schritt vs. Entscheidung ob R√ºckfrage gestellt wird

# Fragen zu dem Einsatz von KI in Informationszugangssystemen?

## Literaturverzeichnis

::: {#refs}
:::